agent:
  policy:
    eps_max: 1.0
    eps_min: 0.1
    eps_test: 0.3
  nb_steps: 100000
  nb_steps_warmup: 1
  nb_max_episode_steps: 200
  nb_max_test_episode_steps: 100
  action_repetition: 3
  target_model_update: 1.0e-2
  gamma: 0.9
  chkpt_interval: 10000
  log_interval: 10
  outdir: "E:/Projects/rogue-rl/output/v5/agent1/run1/"
  train_from_weights:
  model:
    learning_rate: 1.0e-4
    metrics:
      - mae
    layers:
      - name: Conv2D
        kwargs:
          filters: 32
          kernel_size: 8
          strides: 4
          activation: relu
      - name: BatchNormalization
      - name: Conv2D
        kwargs:
          filters: 32
          kernel_size: 4
          strides: 2
          activation: relu
      - name: BatchNormalization
      - name: Flatten
      - name: Dense
        kwargs:
          units: 512
          activation: relu


game:
  screen:
    size:
      - 1920
      - 1080
  state:
    size:
      - 60
      - 40
  video:
    size:
      - 60
      - 40
  zoom: 10
  world:
    size:
      - 60
      - 40
    color:
      - 50
      - 50
      - 50
    min_area_factor: 0.8
    min_segment_dim_factor: 0.2
    max_segment_dim_factor: 0.7
    min_segment_area_factor: 0.1
    max_segment_area_factor: 0.2
  goal:
    size: 3
    color:
      - 0
      - 255
      - 0
  character:
    size: 1
    num_enemies: 5
    enemy_color:
      - 255
      - 0
      - 0
  reward:
    step: 0  # 0.1
    explore: 0  # 1000 / (60 * 40)
    win: 1
    lose: -1
